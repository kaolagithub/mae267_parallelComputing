
%%% use twocolumn and 10pt options with the asme2ej format
\documentclass[twocolumn,10pt]{asme2ej}

\usepackage{epsfig} %% for loading postscript figures
\usepackage{listings}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{pdfpages}
\usepackage{algpseudocode}
\usepackage{courier}
\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}
%\usepackage{multicol}


% Custom colors
\usepackage{color}
\usepackage{listings}
\usepackage{framed}
\usepackage{caption}
\usepackage{bm}
\captionsetup[lstlisting]{font={small,tt}}

\definecolor{mygreen}{rgb}{0,0.6,0}
\definecolor{mygray}{rgb}{0.5,0.5,0.5}
\definecolor{mymauve}{rgb}{0.58,0,0.82}

\lstset{ %
  backgroundcolor=\color{white},   % choose the background color; you must add \usepackage{color} or \usepackage{xcolor}
  basicstyle=\ttfamily\footnotesize, % the size of the fonts that are used for the code
  breakatwhitespace=false,         % sets if automatic breaks should only happen at whitespace
  % breaklines=true,                 % sets automatic line breaking
  captionpos=b,                    % sets the caption-position to bottom
  commentstyle=\color{mygreen},    % comment style
  deletekeywords={...},            % if you want to delete keywords from the given language
  escapeinside={\%*}{*)},          % if you want to add LaTeX within your code
  extendedchars=true,              % lets you use non-ASCII characters; for 8-bits encodings only, does not work with UTF-8
  frame=single,                    % adds a frame around the code
  keepspaces=true,                 % keeps spaces in text, useful for keeping indentation of code (possibly needs columns=flexible)
  columns=flexible,
  keywordstyle=\color{blue},       % keyword style
  language=Python,                 % the language of the code
  morekeywords={*,...},            % if you want to add more keywords to the set
  numbers=left,                    % where to put the line-numbers; possible values are (none, left, right)
  numbersep=5pt,                   % how far the line-numbers are from the code
  numberstyle=\tiny\color{mygray}, % the style that is used for the line-numbers
  rulecolor=\color{black},         % if not set, the frame-color may be changed on line-breaks within not-black text (e.g. comments (green here))
  showspaces=false,                % show spaces everywhere adding particular underscores; it overrides 'showstringspaces'
  showstringspaces=false,          % underline spaces within strings only
  showtabs=false,                  % show tabs within strings adding particular underscores
  stepnumber=1,                    % the step between two line-numbers. If it's 1, each line will be numbered
  stringstyle=\color{mymauve},     % string literal style
  tabsize=4,                       % sets default tabsize to 2 spaces
}



%% The class has several options
%  onecolumn/twocolumn - format for one or two columns per page
%  10pt/11pt/12pt - use 10, 11, or 12 point font
%  oneside/twoside - format for oneside/twosided printing
%  final/draft - format for final/draft copy
%  cleanfoot - take out copyright info in footer leave page number
%  cleanhead - take out the conference banner on the title page
%  titlepage/notitlepage - put in titlepage or leave out titlepage
%
%% The default is oneside, onecolumn, 10pt, final

\title{MAE 267 -- Project 5\\Parallel, Multi-Block, Finite-Volume Methods\\For Solving 2D Heat Conduction}

%%% first author
\author{Logan Halstrom
    \affiliation{
	PhD Graduate Student Researcher\\
	Center for Human/Robot/Vehicle Integration and Performance\\
	Department of Mechanical and Aerospace Engineering\\
	University of California, Davis\\
	Davis, California 95616\\
    Email: ldhalstrom@ucdavis.edu
    }
}

\begin{document}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Statement of Problem}

This analysis demonstrates the fundamentals of parallel computing through the numerical solution of the steady-state, two-dimensional temperature distribution of a 1m x 1m steel block with properties listed in Table~\ref{props}.

%STEEL PROPERTIES TABLE
\vspace{-1.8em}
\begin{table}[htb]
\begin{center}
\caption{Steel Block Properties}
\begin{tabular}{|l | c|}
\hline
Dimensions & 1m x 1m \\
\hline
Thermal Conductivity & $k=18.8\frac{W}{m \cdot K}$  \\
\hline
Density & $\rho=8000\frac{kg}{m^3}$  \\
\hline
Specific Heat Ratio & $c_p=500$  \\
\hline
\end{tabular}
\label{props}
\end{center}
\end{table}
\vspace{-2.3em}

The demonstration of parallel computing techniques was accomplished in stages, starting with a serial (single-processor) solution of a single grid of dimensions 101x101 and 501x501, which serves as a solver basis and performance benchmark for later parallel codes.

The next stage was to divide the grid into NxM subdomains (blocks), on each of which the solution for a given iteration was calculated independently.  5x4 and 10x10 block decompositions of both previous grid dimensions were solved to demonstrate compartmentalization of solver processes, which is a necessary step for distributing processes in parallel computing.

Finally, the code was adapted to solve multi-block decompositions on multiple processors for the 501x501 grid decomposed into 10x10 blocks running on 1 to 8 processors.  For this solution, the domain is decomposed and blocks are distributed on to processors. Decompositions are saved to restart files for each processor to be loaded by each processor in the parallel solver.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Methods and Equations}

The core of this demonstration code is the heat transfer solver developed in the first project, but a number of domain decomposition functions have since been included, as will be detailed in this section.

\subsection{Grid Initialization}

The numerical solution is initialized with the Dirichlet boundary conditions (Eqn~\ref{dirichlet}) using a single processor.

\begin{equation}
\begin{split}
T_{BCs} = &\left\{ \begin{array}{lll}
    \mbox{$5.0 \left[ \sin\left( \pi x_p \right) + 1.0 \right]$} & \mbox{for } &j = j_{max} \\
    \mbox{$\left| \cos\left( \pi x_p \right)\right|+1.0$} & \mbox{for } &j = 0 \\
    \mbox{$3.0 y_p + 2.0$} & \mbox{for } &i = 0, \, i_{max}
     \end{array} \right.
\end{split}
\label{dirichlet}
\end{equation}

\begin{equation}
\begin{split}
   rot &= 30.0 \frac{\pi}{180.0} \\
   x_p(i) &= \cos \left[ 0.5\pi \frac{i_{max}-i}{i_{max}-1} \right] \\
   y_p(j) &= \cos \left[ 0.5\pi \frac{j_{max}-j}{j_{max}-1} \right] \\
   x &= x_p \cos(rot) + (1.0 - y_p) \sin(rot) \\
   y &= y_p \cos(rot) + x_p \sin(rot)
\end{split}
\label{grideqn}
\end{equation}

\noindent Square grids are generated according to Eqn~\ref{grideqn} to create non-uniform spacing in both the x and y directions (with finer spacing at the larger indices).  The ``prime'' system is then rotated by angle $rot$ to create the final grid.

\subsection{Numerical Solver}

The solver developed for this analysis utilizes a finite-volume numerical solution method to solve the transient heat conduction equation (Eqn~\ref{heat}).

\begin{equation}
\begin{split}
\rho c_p \frac{\partial T}{\partial t} =
    k \left[ \frac{\partial^2 T}{\partial x^2}
    + \frac{\partial^2 T}{\partial y^2} \right]
\end{split}
\label{heat}
\end{equation}

\noindent To solve Eqn~\ref{heat} numerically, the equation is discretized according to a node-centered, finite-volume scheme, where first-derivatives at the nodes are found using Green's theorem integrating around the secondary control volumes.  Trapezoidal, counter-clockwise integration for the first-derivative in the x-direction is achieved with Eqn~\ref{FVx1st}.

\begin{equation}
\begin{split}
\frac{\partial T}{\partial x} = \frac{1}{2Vol_{i+\frac{1}{2},j+\frac{1}{2}}}
    \left[ \left(T_{i+1,j} + T_{i+1,j+1} \right)Ayi_{i+1,j} \right. \\
    \left. - \left(T_{i,j} + T_{i,j+1} \right)Ayi_{i,j} \right. \\
    \left. - \left(T_{i,j+1} + T_{i+1,j+1} \right)Ayi_{i,j+1} \right. \\
    \left. - \left(T_{i,j} + T_{i+1,j} \right)Ayi_{i,j} \right]
\end{split}
\label{FVx1st}
\end{equation}

\noindent A similar scheme is used to find the first-derivative in the y-direction.


\subsection{Subdomain Decomposition}

After grid initialization, the grid is divided into N blocks in the x/I direction and M blocks in the y/J direction, creating a total number of blocks $NBLK=N\cdot M$.  All blocks are constrained to have the same number of nodes, so the dimensions of every block $IBLKMAX$ and $JBLKMAX$ are calculated in Eqn~\ref{iblocksize} as a fraction of the total number of nodes in each direction, including one point overlap at each inter-block boundary (Ghost nodes are excluded for the moment).  In the I-direction, the total number of nodes including overlap (Eqn~\ref{idirpoints}) is:

\vspace{-1em}
\begin{equation}
\begin{split}
IMAX_{tot} = IMAX + (N-1)
\end{split}
\label{idirpoints}
\end{equation}
\vspace{-1em}

\noindent and the total number of nodes per block in the I-direction (Eqn~\ref{iblocksize}) is:

\vspace{-1em}
\begin{equation}
\begin{split}
IMAXBLK = \frac{IMAX_{tot}}{N} = \frac{IMAX + (N-1)}{N} = 1 + \frac{IMAX-1}{N}
\end{split}
\label{iblocksize}
\end{equation}

\noindent Note: For points in J-direction, replace I with J and N with M

Blocks are distributed from 1 to NBLK starting in the lower-left corner of the grid and zipping left to right (the x/I/N direction), then up one (the y/J/M direction) starting again at the left.  This is accomplished by two DO loops, the outer loop stepping through J from 1 to M and the inner loop stepping through I from 1 to N. Block locations are stored by assigning global starting indices to each block according to Eqn~\ref{blockstartingindex}.

\begin{equation}
\begin{split}
IMIN_{block} = IMIN_{global} + (IMAXBLK-1)(I-1)
\end{split}
\label{blockstartingindex}
\end{equation}

\noindent where I counts blocks in the direction of N and $IMIN_{global}=1$.  The first block in the N-direction has a global starting index of 0, and IMAXBLK must be reduced by one to account for the single-point overlap at block boundaries.

Information for each block is stored as an element in an array of BLKTYPE derived data types.  BLKTYPE stores local mesh, temperature, and solver information as well as the block ID, global  indices, iteration bounds to prevent overwriting boundary conditions (discussed in Section~\ref{neighborsec}), and neighbor identification information.

\subsection{Processor Distribution}

For the parallel code, blocks are distributed among $NPROCS$ processors (determined in `miprun’ call), with the goal of equal load balancing for all processors (Eqn~\ref{loadbalance}).  Load balance is the ratio of a processor's workload to the ``Perfect Load Balance'' ($PLB$), the total load of all blocks divided by $NPROCS$.  In this code, a block's load is refered to as its $SIZE$, so a processor's work load is equal to the sum of the $SIZEs$ of its blocks.

\vspace{-1em}
\begin{equation}
\begin{split}
P_{LoadBalance} = \frac{SUM(SIZEs)}{PLB}
\end{split}
\label{loadbalance}
\end{equation}

The workload of each block ($SIZE$) is calculated as a weighted sum (Eqn~\ref{blockload}) of its geometric cost $GEOM$ due to grid size (Eqn~\ref{geomcost}) and communication cost $COMM$ due to boundary size (~\ref{commcost}).  Geometric cost is essentially the node area of the block iteration bounds:

\vspace{-1em}
\begin{equation}
\begin{split}
GEOM = (IMAXLOC - IMINLOC) \cdot (JMAXLOC - JMINLOC)
\end{split}
\label{geomcost}
\end{equation}

\noindent Geometric cost will be greater for cells that are not on physical boundaries as they require more ghosts nodes for their inter-block boundaries.  Communication cost is calculated as the total length of all faces and corners at interblock boundaries:

\vspace{-1em}
\begin{equation}
\begin{split}
&COMM(i) =
      \begin{cases}
        0, & \text{if BC} \\
        IMAXBLK - IMINBLK, & \text{if N or S Face Neighbor} \\
        JMAXBLK - JMINBLK, & \text{if E or W Face Neighbor} \\
        1, & \text{if Corner Neighbor}
      \end{cases} \\
&COMM = SUM(COMM(i))
\end{split}
\label{commcost}
\end{equation}
\noindent where Eqn~\ref{commcost} must be evaluated for all faces and corners of a given block and the results must be summed.

Weights of each type of cost are currently set to make the maximum possible geometric cost equal to the maximum possible communication cost, as accomplished by Eqn~\ref{blockload}.

\vspace{-1em}
\begin{equation}
\begin{split}
&WGEOM = 1 \\
&WCOMM = FACTOR \cdot \frac{(IMAXBLK+2)(JMAXBLK+2)}{(2 \cdot IMAXBLK)+(2 \cdot IMAXBLK)+4 } \\
&SIZE = (WGEOM \cdot GEOM) + (WCOMM \cdot COMM)
\end{split}
\label{blockload}
\end{equation}
\noindent where $FACTOR$ is a number that can be varied to tune cost weighting, but is currently set to 1.

Once block loads are calculated, they are sorted by size in order of greatest to least.  They are then distributed to the processors in this order, where each block is assigned to the current processor with the least load.  This produces the theoretical load balancing presented in Section~\ref{results}.  Actual load balancing performance will be determined in Project 5 and tuning will be performed to optimized load balancing.

\subsection{Ghost Nodes and Neighbor Indentification} \label{neighborsec}

In order for each block to function independently for a given iteration of the solver, it must know information about the nodes immediately outside of its boundaries, or, in other words, the interior nodes of its neighbors.  To preserve block independence, each block stores the information it needs from its neighbor at the beginning of each iteration in extra, off-block nodes called “ghost nodes”.  These nodes change the local size of each block and necessitate the local iteration parameters $ILOCMIN$, $ILOCMAX$, etc. discussed earlier.

To update each ghost boundary, the identity of the neighbor block for each face is stored in a variable $NB$, which is a neighbor derived data type $NBRTYPE$, which contains IDs for the north, south, east, and west faces and the north east, south east, south west, and north west corners.  If the block boundary is a physical boundary instead of an inter-block boundary, the corresponding neighbor identifier is instead set to 0 to indicate a BC boundary.  For parallel computing, if a neighbor block is on a different processor (indicating a processor boundary), the neighbor block ID is negated to indicate as such while still preserving the neighbor block ID.

Neighbor information is used to populate a linked list for each boundary type with block IDs so that all similar types of boundaries may be looped through in sequence, rather than using logical sorting at the beginning of each iteration.  (Linked lists were shown to produces a 25\% speed-up compared to logical sorting for the serial, multi-block code).

When moving to parallel computing, the ID of the neighbor block’s processor must also be bookeeped, as it is required information for accessing the neighbor block for ghost updating.  In addition to the neighbor block’s processor, the local index of the neighbor block on it’s processor must also be stored for this same reason.  Thus, this data is stored in corresponding $NBRTYPEs$.  Neighbor processor ID’s are stored in the variable $NP$.  If a block boundary is a BC, the processor ID is negated to indicate as such.  Local indices of neighbor blocks on neighbor processors are stored in $NBLOC$ and are set to 0 if a boundary is a BC.

\subsection{Configuration Restart Files}
After all of the above mentioned initialization processes have been completed, this information is stored in individual restart files for each processor so that the solver may start up independently from these files without needing to determine boundary procedures.  Neighbor information, grid, and temperature files are written \textbf{for each processor}.

\subsection{Parallel Performance}
Performance of the parallel solver is measured in the results section using Amdahl's Law and the concepts of parallel speedup $S_P$ (Eqn~\ref{speedupEqn}) and parallel computational efficiency $E_P$ (Eqn~\ref{effishEqn}).

\vspace{-1em}
\begin{equation}
\begin{split}
S_P = \frac{t_s}{t_p}
\end{split}
\label{speedupEqn}
\end{equation}
\noindent where $t_s$ is the serial solution time and $t_p$ is the parallel solution time.

Ideal speedup produces a line with a slope of 1, with the serial solution time decreasing by exactly the amount of processors used.  Ideal efficiency is 1, and corresponds to the ideal speedup.

\vspace{-1em}
\begin{equation}
\begin{split}
E_P = \frac{S_p}{N_P}
\end{split}
\label{effishEqn}
\end{equation}
\noindent where $N_P$ is the number of processors in the parallel solution.

Speedup can be predicted as a function of the fraction of the code that is run in parallel according to Eqn~\ref{OptimalSpeedup}.  Because only the parallel portion of this code is timed, optimal speedup is expected to by near-ideal.

\vspace{-1em}
\begin{equation}
\begin{split}
S_{P,Opt} = \frac{N_P}{f + (1-f)N_P}
\end{split}
\label{OptimalSpeedup}
\end{equation}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% %%\vspace{-2em}
% \begin{figure}[htb]
% \begin{center}
% \includegraphics[width=0.5\textwidth]{../Results/finalMulti/501_10x10.png}
% \caption{Steady-state temperature solution for 501x501 grid decomposed into 10x10 blocks}
% \label{501x10x10}
% \end{center}
% \end{figure}
% %%\vspace{-2em}

% %%\vspace{-2em}
% \begin{figure}[htb]
% \begin{center}
% \includegraphics[width=0.5\textwidth]{../Results/ResHist.png}
% \caption{Residual history for solving a 501x501 grid with a single block solver and a 10x10 multi-block solver}
% \label{reshist}
% \end{center}
% \end{figure}
% %%\vspace{-2em}

\section{Results and Discussion} \label{results}

Solutions of a 501x501 grid decomposed into 10x10 blocks were solved on 2, 4, 6, and 8 processors on the hpc1 front end (Batch jobs on hpc1 took significantly longer, which seemed to be due to running more processes than allocated cores).

Another important lesson learned from this project was the importance of shared variables in MPI.  Many days of debugging ultimatley resulted in discovering that the global grid size was only read in by Processor 0, and was not available to the other processors for calculation of mesh parameters.  This was solved with a simple MPI\_BCAST.

Another pitfall was in creating unique tag IDs for MPI sends and recieves.  Each send is identified by the sending and recieving processor, as well as a tag, so the first attempt used 8 independent tags for the 8 directions of communication.  This proved to be non-unique as one processor could send to another processor more than once in the same direction for a given iteration.  In the end, a unique tag was created for each send as the concatination of the direction number with the global sending block ID.

%%\vspace{-2em}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.5\textwidth]{../Results/np4_nx501_n10_m10/Composite.png}
\caption{Steady-state heat transfer solution for a 501x501 grid decomposed into 10x10 blocks solved on 4 processors}
\label{Temp}
\end{center}
\end{figure}
%%\vspace{-2em}

%%\vspace{-2em}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.5\textwidth]{../Results/ResHist.pdf}
\caption{Serial and parallel convergence history demonstrating similar performance between all solution methods}
\label{ResHist}
\end{center}
\end{figure}
%%\vspace{-2em}







Convergence history for the serial, single-block and multi-block cases are compared to the parallel case in Fig~\ref{ResHist}.



It can be seen from the convergence comparison that the parallel solver performs almost identically to its serial analogs, and Fig~\ref{Temp} demonstrates that the parallel solution is accurate.



After much deliberation with hpc1, results demonstrating actual parallel speedup were produced and total solution wall times are presented for serial and parallel runs of the Project 5 code in Table~\ref{Times}.


%Solver Times
\vspace{-1.8em}
\begin{table}[htb]
\begin{center}
\caption{\textbf{Serial (Left) and Parallel (Right) Solutions Times}}
\begin{tabular}{|c |c c | c c c c|}
\hline
\textbf{$N_P$} &(1x1) & (10x10) &  2 & 4 & 6  & 8     \\
\hline
 \textbf{t (s)} &360.7  & 377.8 & 179.4 & 91.95 & 70.56 & 76.09 \\
\hline
\end{tabular}
\label{Times}
\end{center}
\end{table}
\vspace{-2.3em}

It can be seen that the parallel code does indeed reach a solution faster than its serial analogs.  The speedup and efficiency of the parallel calculations are further demonstrated in Figures ~\ref{Speedup} and ~\ref{Effish}.










Because near-perfect load balancing was accomplished (shown in Table~\ref{Load}) and because the timed portion of the code was entirely parallel (see Eqn~\ref{OptimalSpeedup}), it was expected that optimal speedups would be near the ideal case, as Fig~\ref{Speedup} demonstrates.



%%\vspace{-2em}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.5\textwidth]{../Results/Speedup.pdf}
\caption{Parallel speedup for a 501x501 grid decomposed into 10x10 blocks demonstrating an ideal speedup and sometimes super-linear that tapers off as the number of processors increases}
\label{Speedup}
\end{center}
\end{figure}
%%\vspace{-2em}

Fig~\ref{Speedup} also demonstrates that the maximum possible speedup for this case is approximately 5.25 running on 6 processors, which is a significant improvement on the performance of the serial version.



The parallel efficiency of the solver is greatest for lower processor numbers.  It actualy exceedes maximum efficiency for the 2 and 4 processor cases, but this phenomenon may be due to variation in loading of the hpc1 front-end cores between runs.



A steep drop-off in efficiency is noted as the number of processors is increased.  To extend the curve further out, the solver could be applied to a larger grid system decomposed into more sub-domains.

%%\vspace{-2em}
\begin{figure}[htb]
\begin{center}
\includegraphics[width=0.5\textwidth]{../Results/Efficiency.pdf}
\caption{Parallel efficiency for a 501x501 grid decomposed into 10x10 blocks demonstrating a greater than ideal efficiency for lower amounts of processors that may be due to variations in core loading}
\label{Effish}
\end{center}
\end{figure}
%%\vspace{-2em}

%LOAD BALANCING
\vspace{-1.8em}
\begin{table}[htb]
\begin{center}
\caption{\textbf{Processor Theoretical Load Balances}}
\begin{tabular}{|c | c c|}
\hline
\textbf{$NPROCS$} & 4   & 8     \\
\hline
\textbf{Proc0}    & 1.0000 & 1.0306 \\
\textbf{Proc1}    & 1.0000 & 1.0306 \\
\textbf{Proc2}    & 1.0000 & 1.0306 \\
\textbf{Proc3}    & 1.0000 & 1.0306 \\
\textbf{Proc4}    & N/A    & 0.96943 \\
\textbf{Proc5}    & N/A    & 0.96943 \\
\textbf{Proc6}    & N/A    & 0.96943 \\
\textbf{Proc7}    & N/A    & 0.96943 \\
\hline
\end{tabular}
\label{Load}
\end{center}
\end{table}
\vspace{-2.3em}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Conclusion}

The product of this project is a parallel, multi-block heat conduction solver demonstrating the fundamental principles of computations in parallel.  It increased knowledge of parallel information passing, parallel speedup performance, load balancing optimization.

The working code is just a beginning of what can be accomplished with parallel computing.  This code itself could be improved with better sub-domain decomposition optimization and optimization of calculations like send/recieve tags.  Beyond that, the principles of this code could be applied to numerical solvers of different principles such as fluid dynamics or structures.  Parallel computing could be adapted to GPUs instead of CPU cores.  The list is endless.

These projects served as an effective introduction to parallel computing by gradually building on principles; starting with a standard numeric heat transfer solver and then incrementally incorperating parallel computing concepts like domain decomposition and solution restart files so that each concept could be solidified before the next was attempted.  Aside from developing a strong parallel computing basis from which to build off of, the course also developed skills for aquiring further knowledge, leaving the student prepared to delve further into the field of parallel computing.

This course is highly recommended for students interested in numerical solutions of engineering problems, as the principles of these projects can be adapted to any application that may be of interest.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{References}
% \begin{description}
% \item[] 1. Bogard, D.G., Teiderman, W.G. ``Burst detection with single-point velocity measurements'', \emph{Journal of Fluid Mechanics}, 162:389-413, 1986.
% \end{description}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \clearpage
\onecolumn
\appendix       %%% starting appendix

\section*{Appendix A: Parallel, Multi-Block Grid Wrapper Code}
\lstinputlisting[caption=Wrapper program that performs domain decomposition on a single processor and then independently reads in solution for solver, language=Fortran]{../main.f90}

\clearpage

\section*{Appendix B: Parallel, Multi-Block Grid Solver Code}
\lstinputlisting[caption=Grid decomposition and solution processess are organized in this code, language=Fortran]{../subroutines.f90}

\section*{Appendix C: Parallel, Multi-Block Grid Decomposition Code}
\lstinputlisting[caption=Grids are decomposed into blocks and mapped onto NPROCS processors and information pertaining to neighbors is stored using the GRIDMOD module, language=Fortran]{../modules.f90}

\clearpage

\section*{Appendix D: Multi-Block Plot3D Reader-Writer}
\lstinputlisting[caption=Code for saving formatted multiblock PLOT3D solution files and reading restart files, language=Fortran]{../inout.f90}

% \clearpage




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\clearpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% The bibliography is stored in an external database file
% in the BibTeX format (file_name.bib).  The bibliography is
% created by the following command and it will appear in this
% position in the document. You may, of course, create your
% own bibliography by using thebibliography environment as in
%
% \begin{thebibliography}{12}
% ...
% \bibitem{itemreference} D. E. Knudsen.
% {\em 1966 World Bnus Almanac.}
% {Permafrost Press, Novosibirsk.}
% ...
% \end{thebibliography}

% Here's where you specify the bibliography style file.
% The full file name for the bibliography style file
% used for an ASME paper is asmems4.bst.
%\bibliographystyle{asmems4}

% Here's where you specify the bibliography database file.
% The full file name of the bibliography database for this
% article is asme2e.bib. The name for your database is up
% to you.
%\bibliography{asme2e}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\end{document}
